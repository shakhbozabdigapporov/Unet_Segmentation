{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcd88ae1-d0fd-406c-9982-275b62cd7b69",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf37362-a4b3-40f5-9bf9-3a2aa86b9cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been split and copied successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "img_path = \"Your Images Path\"\n",
    "mask_path = \"Your masks Path\"\n",
    "\n",
    "\n",
    "images = sorted(os.listdir(img_path))\n",
    "masks = sorted(os.listdir(mask_path))\n",
    "\n",
    "assert len(images) == len(masks), \"The number of images and masks should be equal.\"\n",
    "\n",
    "combined = list(zip(images, masks))\n",
    "random.shuffle(combined)\n",
    "images[:], masks[:] = zip(*combined)\n",
    "\n",
    "total_images = len(images)\n",
    "train_end = int(total_images * 0.8)\n",
    "val_end = train_end + int(total_images * 0.1)\n",
    "\n",
    "train_images = images[:train_end]\n",
    "train_masks = masks[:train_end]\n",
    "\n",
    "val_images = images[train_end:val_end]\n",
    "val_masks = masks[train_end:val_end]\n",
    "\n",
    "test_images = images[val_end:]\n",
    "test_masks = masks[val_end:]\n",
    "\n",
    "def copy_files(file_list, source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    for file in file_list:\n",
    "        shutil.copy(os.path.join(source_dir, file), os.path.join(target_dir, file))\n",
    "\n",
    "\n",
    "train_img_dir = \"Your path/train/img/img\"\n",
    "train_mask_dir = \"Your path/train/mask/img\"\n",
    "val_img_dir = \"Your path/val/img/img\"\n",
    "val_mask_dir = \"Your path/val/mask/img\"\n",
    "test_img_dir = \"Your path/test/img/img\"\n",
    "test_mask_dir = \"Your path/test/mask/img\"\n",
    "\n",
    "# Copy files to target directories\n",
    "copy_files(train_images, img_path, train_img_dir)\n",
    "copy_files(train_masks, mask_path, train_mask_dir)\n",
    "\n",
    "copy_files(val_images, img_path, val_img_dir)\n",
    "copy_files(val_masks, mask_path, val_mask_dir)\n",
    "\n",
    "copy_files(test_images, img_path, test_img_dir)\n",
    "copy_files(test_masks, mask_path, test_mask_dir)\n",
    "\n",
    "print(\"Files have been split and copied successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48232e3c-550d-422d-b8aa-3243976dcd21",
   "metadata": {},
   "source": [
    "# Start Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2c94952-b0ef-4f81-8c20-3ccec1fc5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb52ef34-ed83-46a0-a98d-c153cc27ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "SEED = 909\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "IMG_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "data_dir = 'Processed data directory with tran|val|test folders'\n",
    "data_dir_train = os.path.join(data_dir, 'train')\n",
    "data_dir_train_image = os.path.join(data_dir_train, 'img')\n",
    "data_dir_train_mask = os.path.join(data_dir_train, 'mask')\n",
    "\n",
    "data_dir_test = os.path.join(data_dir, 'val')\n",
    "data_dir_test_image = os.path.join(data_dir_test, 'img')\n",
    "data_dir_test_mask = os.path.join(data_dir_test, 'mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8636aa07-62bd-4e03-8ef1-830eb17d537c",
   "metadata": {},
   "source": [
    "## Segmentation data generator with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a65ce1b-35b3-4976-9e1e-c881dcd8b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "\n",
    "IMG_SIZE = (512, 512) \n",
    "SEED = 42 \n",
    "\n",
    "def elastic_transform(image, mask, alpha, sigma):\n",
    "    \"\"\"Applies elastic deformation to an image and mask.\"\"\"\n",
    "    random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape[:2]\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1))\n",
    "\n",
    "    distorted_image = map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n",
    "    distorted_mask = map_coordinates(mask, indices, order=1, mode='reflect').reshape(shape)\n",
    "\n",
    "    return distorted_image, distorted_mask\n",
    "\n",
    "def apply_augmentation(img, msk):\n",
    "    \"\"\"Applies data augmentation including elastic_transform, flips.\"\"\"\n",
    "    if random.random() > 0.5:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        msk = tf.image.flip_left_right(msk)\n",
    "    if random.random() > 0.5:\n",
    "        img = tf.image.flip_up_down(img)\n",
    "        msk = tf.image.flip_up_down(msk)\n",
    "\n",
    "    return img, msk\n",
    "\n",
    "def preprocess_data(img, msk):\n",
    "    \"\"\"Rescale and apply augmentation.\"\"\"\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    msk = tf.image.resize(msk, IMG_SIZE)\n",
    "    \n",
    "    img, msk = apply_augmentation(img, msk)\n",
    "\n",
    "    img = img / 255.0\n",
    "    msk = msk / 255.0\n",
    "\n",
    "    return img, msk\n",
    "\n",
    "def create_segmentation_generator(img_path, msk_path, BATCH_SIZE):\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    img_generator = datagen.flow_from_directory(\n",
    "        img_path,\n",
    "        target_size=IMG_SIZE,\n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=SEED\n",
    "    )\n",
    "    msk_generator = datagen.flow_from_directory(\n",
    "        msk_path,\n",
    "        target_size=IMG_SIZE,\n",
    "        class_mode=None,\n",
    "        color_mode='grayscale',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        seed=SEED\n",
    "    )\n",
    "    while True:\n",
    "        imgs = next(img_generator)\n",
    "        msks = next(msk_generator)\n",
    "        yield imgs, msks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fdfd1d2f-7cc8-4acd-ade1-c0278abe5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    " def create_segmentation_generator_test(img_path, msk_path, BATCH_SIZE):\n",
    "    data_gen_args = dict(rescale=1./255)\n",
    "    datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    img_generator = datagen.flow_from_directory(\n",
    "        img_path, \n",
    "        target_size=IMG_SIZE, \n",
    "        class_mode=None, \n",
    "        color_mode='grayscale', \n",
    "        batch_size=BATCH_SIZE, \n",
    "        seed=SEED\n",
    "    )\n",
    "    msk_generator = datagen.flow_from_directory(\n",
    "        msk_path, \n",
    "        target_size=IMG_SIZE, \n",
    "        class_mode=None, \n",
    "        color_mode='grayscale', \n",
    "        batch_size=BATCH_SIZE, \n",
    "        seed=SEED\n",
    "    )\n",
    "    while True:\n",
    "        imgs = next(img_generator)\n",
    "        msks = next(msk_generator)\n",
    "        yield imgs, msks\n",
    "\n",
    "def create_segmentation_generator_train(img_path, msk_path, BATCH_SIZE):\n",
    "    data_gen_args = dict(rescale=1./255)\n",
    "    datagen = ImageDataGenerator(**data_gen_args)\n",
    "    img_generator = datagen.flow_from_directory(\n",
    "        img_path, \n",
    "        target_size=IMG_SIZE, \n",
    "        class_mode=None, \n",
    "        color_mode='grayscale', \n",
    "        batch_size=BATCH_SIZE, \n",
    "        seed=SEED\n",
    "    )\n",
    "    msk_generator = datagen.flow_from_directory(\n",
    "        msk_path, \n",
    "        target_size=IMG_SIZE, \n",
    "        class_mode=None, \n",
    "        color_mode='grayscale', \n",
    "        batch_size=BATCH_SIZE, \n",
    "        seed=SEED\n",
    "    )\n",
    "    while True:\n",
    "        imgs = next(img_generator)\n",
    "        msks = next(msk_generator)\n",
    "        yield imgs, msks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c50253ed-8bc4-44bf-8966-0889b73af243",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = create_segmentation_generator(data_dir_train_image, data_dir_train_mask, BATCH_SIZE_TRAIN)\n",
    "test_generator = create_segmentation_generator(data_dir_test_image, data_dir_test_mask, BATCH_SIZE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8b07bc8f-05be-44dc-a34e-3e060f11a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d494fc6-895f-4e2c-907d-643462a889c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dataset(datagen, num=1):\n",
    "    for i in range(0,num):\n",
    "        image,mask = next(datagen)\n",
    "        display([image[0], mask[0]])\n",
    "show_dataset(test_generator, num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0cc152d0-f588-4227-a567-433b1eea32b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(n_levels, initial_features=32, n_blocks=2, kernel_size=3, pooling_size=2, in_channels=1, out_channels=1):\n",
    "    inputs = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, in_channels))\n",
    "    x = inputs\n",
    "    \n",
    "    convpars = dict(kernel_size=kernel_size, activation='relu', padding='same')\n",
    "    \n",
    "    #downstream\n",
    "    skips = {}\n",
    "    for level in range(n_levels):\n",
    "        for _ in range(n_blocks):\n",
    "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convpars)(x)\n",
    "        if level < n_levels - 1:\n",
    "            skips[level] = x\n",
    "            x = keras.layers.MaxPool2D(pooling_size)(x)\n",
    "            \n",
    "    # upstream\n",
    "    for level in reversed(range(n_levels-1)):\n",
    "        x = keras.layers.Conv2DTranspose(initial_features * 2 ** level, strides=pooling_size, **convpars)(x)\n",
    "        x = keras.layers.Concatenate()([x, skips[level]])\n",
    "        for _ in range(n_blocks):\n",
    "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convpars)(x)\n",
    "            \n",
    "    # output\n",
    "    activation = 'sigmoid' if out_channels == 1 else 'softmax'\n",
    "    x = keras.layers.Conv2D(out_channels, kernel_size=1, activation=activation, padding='same')(x)\n",
    "    return keras.Model(inputs=[inputs], outputs=[x], name=f'UNET-L{n_levels}-F{initial_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa0d252f-a4a3-4042-ad7b-783b12b9b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "model_path = \"model_checkpoint.h5\"\n",
    "callbacks = [\n",
    "        ModelCheckpoint(model_path, verbose=1, save_best_only=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.0001, verbose=1),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5ac9b60c-5791-46ff-aff2-a9ae7bc510cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1e-15\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true = tf.keras.layers.Flatten()(y_true)\n",
    "    y_pred = tf.keras.layers.Flatten()(y_pred)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "89615d0c-b389-4759-b232-dc60a2f41e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "NUM_TRAIN = 7694\n",
    "NUM_TEST = 961\n",
    "EPOCH_STEP_TRAIN = NUM_TRAIN // BATCH_SIZE_TRAIN\n",
    "EPOCH_STEP_TEST = NUM_TEST // BATCH_SIZE_TEST\n",
    "\n",
    "model = unet(4)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coef,'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4afcf821-3983-41c0-b62d-649ab3b81821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"UNET-L4-F32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 512, 512, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 512, 512, 32  320         ['input_4[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 512, 512, 32  9248        ['conv2d_45[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 256, 256, 32  0          ['conv2d_46[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 256, 256, 64  18496       ['max_pooling2d_9[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 256, 256, 64  36928       ['conv2d_47[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 128, 128, 64  0          ['conv2d_48[0][0]']              \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 128, 128, 12  73856       ['max_pooling2d_10[0][0]']       \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 128, 128, 12  147584      ['conv2d_49[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 64, 64, 128)  0          ['conv2d_50[0][0]']              \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 64, 64, 256)  295168      ['max_pooling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 64, 64, 256)  590080      ['conv2d_51[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_transpose_9 (Conv2DTran  (None, 128, 128, 12  295040     ['conv2d_52[0][0]']              \n",
      " spose)                         8)                                                                \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 128, 128, 25  0           ['conv2d_transpose_9[0][0]',     \n",
      "                                6)                                'conv2d_50[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 128, 128, 12  295040      ['concatenate_9[0][0]']          \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 128, 128, 12  147584      ['conv2d_53[0][0]']              \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_transpose_10 (Conv2DTra  (None, 256, 256, 64  73792      ['conv2d_54[0][0]']              \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 256, 256, 12  0           ['conv2d_transpose_10[0][0]',    \n",
      "                                8)                                'conv2d_48[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 256, 256, 64  73792       ['concatenate_10[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 256, 256, 64  36928       ['conv2d_55[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_11 (Conv2DTra  (None, 512, 512, 32  18464      ['conv2d_56[0][0]']              \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenate)   (None, 512, 512, 64  0           ['conv2d_transpose_11[0][0]',    \n",
      "                                )                                 'conv2d_46[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 512, 512, 32  18464       ['concatenate_11[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 512, 512, 32  9248        ['conv2d_57[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 512, 512, 1)  33          ['conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,140,065\n",
      "Trainable params: 2,140,065\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e89d673-32f6-4a6e-a933-7a4066c7f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_generator, \n",
    "                    steps_per_epoch=EPOCH_STEP_TRAIN, \n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=EPOCH_STEP_TEST,\n",
    "                    callbacks=callbacks,\n",
    "                    epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e3d1e0b5-a222-49cf-9a63-246295fdd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def plot_metric(history, metric, save_path):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history.history[metric], label=f\"Train {metric}\")\n",
    "        plt.plot(history.history[f\"val_{metric}\"], label=f\"Validation {metric}\")\n",
    "        plt.title(f\"Training and Validation {metric.capitalize()}\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(metric.capitalize())\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13879e66-ea67-4b97-9d31-b42261f7a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path_loss = \"Path/loss.png\"\n",
    "save_path_accuracy = \"Path/accuracy.png\"\n",
    "save_path_dice = \"Path/dice.png\"\n",
    "plot_metric(history, \"loss\", save_path_loss)\n",
    "plot_metric(history, \"dice_coef\", save_path_dice)\n",
    "plot_metric(history, \"accuracy\", save_path_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66158566-d844-4209-a358-78be2a9a0ab9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('Path/trained_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6d83c-459e-4e01-b13d-6926219ef898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e328d853-462c-4597-987a-4c9c19a8e84a",
   "metadata": {},
   "source": [
    "# Testing your Prediction with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1e2429f-7b2b-45a6-b0c6-8f9ef8422f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "savedModelPath = 'Weight_file.h5'\n",
    "f = h5py.File(savedModelPath, mode=\"r+\")\n",
    "model_config_string = f.attrs.get(\"model_config\")\n",
    "if model_config_string.find('\"groups\": 1,') != -1:\n",
    "    model_config_string = model_config_string.replace('\"groups\": 1,', '')\n",
    "    f.attrs.modify('model_config', model_config_string)\n",
    "    f.flush()\n",
    "    model_config_string = f.attrs.get(\"model_config\")\n",
    "    assert model_config_string.find('\"groups\": 1,') == -1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9547f4e9-5ef3-4377-9c33-18915cf62706",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "bb5db097-063a-4a2f-80f0-7980248eb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Weight_file.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "00581b51-bde7-418f-b8ca-785cc64e8de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_image(image_path, target_size):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img = img.convert('L')\n",
    "    \n",
    "    # Resize the image\n",
    "    img = img.resize(target_size)\n",
    "    \n",
    "    # Convert the image to a numpy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Normalize the image (if normalization was used during training)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Expand dimensions to match the model's expected input shape\n",
    "    img_array = np.expand_dims(img_array, axis=-1)  # Add the channel dimension\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add the batch dimension\n",
    "    \n",
    "    return img_array\n",
    "    \n",
    "# Example usage\n",
    "target_size = (512, 512)  # Set this to the size your model expects\n",
    "image_path = 'image.png'\n",
    "img_array = load_and_preprocess_image(image_path, target_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "57df82ee-a5da-4d4f-8acc-1d7b54d5df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_mask = model.predict(img_array)\n",
    "predicted_mask = np.squeeze(predicted_mask, axis=0) \n",
    "predicted_mask = (predicted_mask > 0.5).astype(np.uint8)\n",
    "original_image = Image.open(image_path)\n",
    "original_size = original_image.size\n",
    "mask_image = Image.fromarray(predicted_mask.squeeze())\n",
    "mask_image = mask_image.resize(original_size, Image.NEAREST)\n",
    "mask_image.save('test.png')\n",
    "\n",
    "# pred_mask = (model.predict(image_path)[0] > 0.5).astype(np.uint8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet_seg",
   "language": "python",
   "name": "unet_seg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
